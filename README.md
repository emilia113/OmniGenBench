<!-- start -->

<div align="center">
<img src="assets/omnigenbench.png" alt="OmniGenBench Logo" width="58">

<h2>OmniGenBench: A Benchmark for Omnipotent Multimodal Generation across 50+ Tasks</h2>

<a href="https://github.com/emilia113">Jiayu Wang</a><sup>1,2*</sup>&nbsp;
<a href="https://sxjyjay.github.io/">Yang Jiao</a><sup>1,2*</sup>&nbsp;
<a href="https://github.com/Yue-105">Yue Yu</a><sup>1,2</sup>&nbsp;
<a href="https://qiantianwen.github.io/">Tianwen Qian</a><sup>3</sup>&nbsp;
<br>
<a href="https://scholar.google.com/citations?user=WL5mbfEAAAAJ&hl=zh-CN">Shaoxiang Chen</a><sup>4</sup>&nbsp;
<a href="https://jingjing1.github.io/">Jingjing Chen</a><sup>1,2‚Ä†</sup>&nbsp;
<a href="https://fvl.fudan.edu.cn/">Yu-Gang Jiang</a><sup>1,2</sup>

<sup>1</sup>Shanghai Key Lab of Intell. Info. Processing, School of CS, Fudan University<br>
<sup>2</sup>Shanghai Collaborative Innovation Center on Intelligent Visual Computing<br>
<sup>3</sup>School of Computer Science and Technology, East China Normal University<br>
<sup>4</sup>Minimax  

<!-- Uncomment below to show paper and huggingface badge -->
[![OmniGenBench](https://img.shields.io/badge/Paper-OmniGenBench-d32f2f.svg?logo=arXiv)](https://arxiv.org/abs/2505.18775)&#160;
[![OmniGenBench](https://img.shields.io/badge/%F0%9F%A4%97%20HF%20-OmniGenBench-blue)](https://huggingface.co/datasets/emiliiia/OmniGenBench)


</div>
<br>

## üì¢ News


- **[2025/05/27]** Our paper is released.
- **[2025/05/26]** The benchmark and evaluation code will be released soon.

<br>
<br>

## üìù TODO

- [ ] Release leaderboard
- [ ] Release evaluation code
- [x] Release benchmark data

<br>
<br>

## üí° Introduction

</div>
<div align="center">
  <img src="assets/class.png" width="92%">
</div>
With <b>50+ diverse sub-tasks</b>, OmniGenBench serves as a novel and comprehensive benchmark for evaluating the generality, adaptability, and reasoning capabilities of state-of-the-art generative models across both perception- and cognition-oriented generation tasks.
<br><br>
OmniGenBench ensures task diversity and difficulty by building on MegaBench, a widely acknowledged multimodal benchmark. We reverse-engineer text queries from its tasks and apply human filtering for accuracy and challenge.
<br><br>
To enable precise evaluation, we design dedicated evaluation protocols for both perception- and cognition-centric tasks. Each task is assigned a tailored evaluation criterion, and our protocol aligns closely with human judgment, ensuring both accuracy and consistency in performance assessment.


<br>
<br>

## üõ†Ô∏è Quick Start

Evaluation data and toolkit will be available shortly.

<br>
<br>

## üé® Model Outputs Showcase
<div align="center">
  <img src="assets/exper.png" width="100%">
</div>
